{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with a single neuron using Flux.jl\n",
    "\n",
    "In this notebook, we'll use `Flux` to create a single neuron and teach it to learn, as we did by hand in notebook 10!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data and process it\n",
    "\n",
    "Let's start by reading in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg; Pkg.add(\"CSV\"); \n",
    "# Pkg.add(\"TextParse\"); Pkg.add(\"DataFrames\")\n",
    "using CSV\n",
    "using TextParse\n",
    "using DataFrames\n",
    "\n",
    "applecols, applecolnames = TextParse.csvread(\"data/Apple_Golden_1.dat\", '\\t')\n",
    "bananacols, bananacolnames = TextParse.csvread(\"data/bananas.dat\", '\\t')\n",
    "\n",
    "apples = DataFrame(Dict([strip(name)=>col for (name, col) in zip(applecolnames, applecols)]...))\n",
    "bananas = DataFrame(Dict([strip(name)=>col for (name, col) in zip(bananacolnames, bananacols)]...));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and processing it to extract information about the red and green coloring in our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = :red\n",
    "col2 = :green\n",
    "\n",
    "x_apples  = [ [apples[i, col1], apples[i, col2]] for i in 1:size(apples)[1] ]\n",
    "x_bananas = [ [bananas[i, col1], bananas[i, col2]] for i in 1:size(bananas)[1] ]\n",
    "\n",
    "xs = vcat(x_apples, x_bananas)\n",
    "\n",
    "ys = vcat( zeros(size(x_apples)[1]), ones(size(x_bananas)[1]) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is in `xs` and the labels (true classifications as bananas or apples) in `ys`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `Flux.jl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load `Flux` to really get going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg; Pkg.add(\"Flux\")\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the last notebook that σ is a built-in function in `Flux`.\n",
    "\n",
    "Another function that is used a lot in neural networks is called `ReLU`; in Julia, the function is called `relu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Use the docs to discover what `ReLU` is all about.\n",
    "\n",
    "`relu.([-3, 3])` returns\n",
    "\n",
    "A) [-3, 3] <br>\n",
    "B) [0, 3] <br>\n",
    "C) [0, 0] <br>\n",
    "D) [3, 3] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "\n",
    "If you run\n",
    "\n",
    "```julia\n",
    "?relu\n",
    "```\n",
    "\n",
    "you'll see that `relu` returns `0` for nonpositive input values and the input value itself for positive input values. Therefore `relu.([-3, 3])` will return (B) `[0, 3]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a single neuron in Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `Flux` to build our neuron with 2 inputs and 1 output:\n",
    "\n",
    " <img src=\"data/single-neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously put the two weights in a vector, $\\mathbf{w}$. Flux instead puts weights in a $1 \\times 2$ matrix (i.e. a matrix with 1 *row* and 2 *columns*). \n",
    "\n",
    "Previously, to compute the dot product of $\\mathbf{w}$ and $\\mathbf{x}$ we had to use either the `dot` function, or we had to transpose the vector $\\mathbf{w}$:\n",
    "\n",
    "```julia\n",
    "# transpose w\n",
    "b = w' * x\n",
    "# or use dot!\n",
    "b = dot(w, x)\n",
    "```\n",
    "If the weights are instead stored in a $1 \\times 2$ matrix, `W`, then we can simply multiply `W` and `x` together instead!\n",
    "\n",
    "We start off with random values for our parameters and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = rand(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the product of `W` and `x` will now be an array (vector) with a single element, rather than a single number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our bias `b` is treated as an array when we're using `Flux`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = rand(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Write a function `mypredict` that will take a single input, array `x` and use `W`, `b`, and built-in `σ` to generate an output prediction (stored as an array). This function defines our neural network!\n",
    "\n",
    "Hint: This function will look very similar to $f_{\\mathbf{w},\\mathbf{b}}$ from the last notebook but has changed since our data structures to store our parameters have changed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "\n",
    "```julia\n",
    "mypredict(x) = σ.(W*x + b)\n",
    "```\n",
    "\n",
    "**Test**\n",
    "\n",
    "```julia\n",
    "W = rand(1, 2)\n",
    "x = rand(2)\n",
    "b = rand(1)\n",
    "\n",
    "isapprox(mypredict(x), σ.(W*x + b))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypredict(x) = σ.(W*x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Define a loss function called `loss`.\n",
    "\n",
    "`loss` should take two inputs: a vector storing data, `x`, and a vector storing the correct \"labels\" for that data. `loss` should return the sum of the squares of differences between the predictions and the correct labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "\n",
    "```julia\n",
    "loss(x, y) = sum((mypredict(x) .- y).^2)\n",
    "```\n",
    "\n",
    "**Tests**\n",
    "\n",
    "\n",
    "```julia\n",
    "x, y = rand(2), rand(1) \n",
    "isapprox( loss(x, y), sum((mypredict(x) .- y).^2) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = sum((mypredict(x) .- y).^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating gradients using Flux: backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning, we know that what we need is a way to calculate derivatives of the `loss` function with respect to the parameters `W` and `b`. So far, we have been doing that using finite differences. \n",
    "\n",
    "`Flux.jl` instead implements a numerical method called **backpropagation** that calculates gradients (essentially) exactly, in an automatic way, by indirectly applying the rules of calculus.\n",
    "To do so, it provides a new type of object called \"tracked\" arrays. These are arrays that store not only their current value, but also information about gradients, which is used by the backpropagation method.\n",
    "\n",
    "[If you want to understand the maths behind backpropagation, we recommend e.g. [this lecture](https://www.youtube.com/watch?v=i94OvYb6noo).]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, `Flux` provides a function `param` to define such objects that will contain the information for a *param*eter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start, as usual, by setting up some random initial values for the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_data = rand(1, 2)  \n",
    "b_data = rand(1)\n",
    "\n",
    "W_data, b_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up `Flux.jl` objects that will contain these values *and* their derivatives, and allow to propagate\n",
    "this information around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = param(W_data)\n",
    "b = param(b_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `param` is a function that `Flux` provides to create an object that represents a parameter of a machine learning model, i.e. an object which has both a value and derivative information, and such that other objects know how to *keep track* of when it is used in an expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "What type does `W` have?\n",
    "\n",
    "A) Array (1D) <br>\n",
    "B) Array (2D) <br>\n",
    "C) TrackedArray (1D) <br>\n",
    "D) TrackedArray (2D) <br>\n",
    "E) Parameter (1D) <br>\n",
    "F) Parameter (2D) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "D) `TrackedArray` (2D)\n",
    "\n",
    "```julia\n",
    "typeof(W)\n",
    "```\n",
    "gives `TrackedArray{…,Array{Float64,2}}`. This can be interpreted as `TrackedArray{…,Array{T,N}}` where `T` is the type and `N` is the dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "`W` stores not only its current value, but also has space to store gradient information. You can access the values and gradient of the weights as follows:\n",
    "\n",
    "```julia\n",
    "W.data\n",
    "W.grad\n",
    "```\n",
    "\n",
    "At this point, are the values of the weights or the gradient of the weights larger?\n",
    "\n",
    "A) the values of the weights <br>\n",
    "B) the gradient of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "A) the values of the weights.\n",
    "\n",
    "We've randomly initialized the data in `W` with values between `0` and `1`. When we use `params` to create a `TrackedArray`, the gradient is initialized to `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "For data `x` and `y` where\n",
    "\n",
    "```julia\n",
    "x, y = [0.413759, 0.692204], [0.845677]\n",
    "```\n",
    "apply the loss function to `x` and `y` to give a new variable `l`. What is the type of `l`? (How many dimensions does it have?)\n",
    "\n",
    "A) Array (0D) <br>\n",
    "B) Array (1D) <br>\n",
    "C) TrackedArray (0D) <br>\n",
    "D) TrackedArray (1D)<br> \n",
    "E) Float64<br>\n",
    "F) Int64<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "C) `TrackedArray` (0D) (Note that this means that `l` is just a single number!)\n",
    "\n",
    "```julia\n",
    "mypredict(x) = σ.(W*x + b)\n",
    "loss(x, y) = sum((mypredict(x) .- y).^2)\n",
    "x, y = [0.413759, 0.692204], [0.845677]\n",
    "l = loss(x, y)\n",
    "typeof(l)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [0.413759, 0.692204], [0.845677]\n",
    "l = loss(x, y)\n",
    "typeof(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having set up the structure, we can now **propagate information about derivatives backwards ** from the `loss` function to all of the objects that are used to calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Tracker\n",
    "\n",
    "back!(l)   # backpropagate derivatives of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we can look at the derivatives again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these results? They are the components of the **gradient of the loss function** with respect to each component of the object `W`, and with respect to `b`! So as promised, `Flux` has done the hard work of calculating derivatives for us!\n",
    "\n",
    "*Bonus info*:\n",
    "\n",
    "To do so, internally Flux sets up a \"computational graph\" and propagates the information on derivatives backwards through the graph. Each node of the graph knows which objects feed into that node, so it tells them to also update their gradients, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these features to reimplement stochastic gradient descent, following the method we used in the previous notebook, but now using backpropagation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "Modify the code from the previous notebook for stochastic gradient descent to use Flux instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [0.413759, 0.692204], [0.845677]\n",
    "loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function stochastic_gradient_descent(loss, W, b, xs, ys, N=1000)\n",
    "\n",
    "    η = 0.01\n",
    "    \n",
    "    for step in 1:N\n",
    "\n",
    "        i = rand(1:length(xs))  # choose a data point\n",
    "\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        l = loss(x, y)\n",
    "        back!(l)\n",
    "        b.data .-= η * b.grad\n",
    "        W.data .-= η * W.grad\n",
    "    end\n",
    "    \n",
    "    return W, b\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the values stored in `b` before we run stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `stochastic_gradient_descent`, we find the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_final, b_final = stochastic_gradient_descent(loss, W, b, xs, ys, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can look at the values of `W_final` and `b_final`, which our machine learned to generate our desired classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "\n",
    "Plot the data and the learned function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg; Pkg.add(\"Plots\")\n",
    "using Plots; gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the function that the network has learned, together with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(0:0.01:1, 0:0.01:1, (x,y)->mypredict([x, y]).data[1])\n",
    "\n",
    "scatter!(first.(x_apples), last.(x_apples), m=:cross)\n",
    "scatter!(first.(x_bananas), last.(x_bananas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9\n",
    "\n",
    "Do this plot every so often as the learning process is proceeding in order to have an animation of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation with Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to repeat the above process for a lot of different systems.\n",
    "Fortunately, `Flux.jl` provides us with tools to automate this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux allows to create a neuron in a simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg; Pkg.add(\"Flux\")\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2, 1, σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `2` and `1` refer to the number of inputs and outputs, and the neuron is defined using the $\\sigma$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made an object of type `Dense`, defined by `Flux`, with the name `model`. This represents a \"dense neural network layer\" (see later for more on neural network layers).\n",
    "The parameters that will be modified during the learning process live *inside* the `model` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10\n",
    "\n",
    "Investigate which variables live inside the `model` object and what type they are. How does that compare to the call to create the `Dense` object that we started with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `model.<TAB>` (i.e. write `model.` and then press the `TAB` key to do autocompletion) to check interactively what is inside the `model` object, or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that `model.W` and `model.b` are of size $1 \\times 2$ and $1$, respectively, comes from the `(2, 1)` pair in the call to the `Dense` constructor when we created `model`.\n",
    "\n",
    "`model.W` will be multiplied by a vector `x` of length 2, which it is why it needs to be of size $1 \\times 2$.\n",
    "\n",
    "Again, these are tracked arrays so that Flux can calculate their gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model object as a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the `model` object to data just as if it were a standard function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(rand(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11\n",
    "\n",
    "Prove to yourself that you understand what is going on when we call `model`. Create two arrays `W` and `b` with the same elements as `model.W` and `model.b`. Use `W` and `b` to generate the same answer that you get when we call `model([.5, .5])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to provide Flux with three pieces of information: \n",
    "\n",
    "1. A loss function\n",
    "2. Some training data\n",
    "3. An optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux has various loss functions built in, for example the mean-squared error (`mse`) that we have been using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = Flux.mse(model(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common one is the cross entropy, `Flux.crossentropy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can take a couple of different forms. \n",
    "One form is a single **iterator**, consisting of pairs $(x, y)$ of data and labels.\n",
    "We can achieve this with `zip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 12\n",
    "\n",
    "Use `zip` to \"zip together\" `xs` and `ys`. Then use the `collect` function to check what `zip` actually does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to tell Flux what kind of optimization routine to use. It has several built in; the standard stochastic gradient descent algorithm that we have been using is called `SGD`. We must pass it two things: a list of parameter objects which will be modified by the optimization routine, and a step size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD([model.W, model.b], 0.01)\n",
    "# give a list of the parameters that will be modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient calculations and parameter updates will be carried out by this optimizer function; we do not see those details, but if you are curious, you can, of course, look at the `Flux.jl` source code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces in place to actually **train** our model (a single neuron) on the data. \n",
    "\"Training\" refers to using pre-labeled data to learn the function that relates the input data to the desired output data given by the labels.\n",
    "\n",
    "`Flux` provides the function `train!`, which performs a single pass through the data and does a single step of optimization using the partial cost function for each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, data, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then just repeat this several times to train the network more and coax it towards the minimum of the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:100\n",
    "    Flux.train!(loss, data, opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the parameters after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing out a list of parameters to modify, `Flux` provides the function `params`, which extracts all available parameters from a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(params(model), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13\n",
    "\n",
    "So far we have just used two features, red and green. \n",
    "\n",
    "(i) Add a third feature, blue. Plot the new data.\n",
    "\n",
    "(ii) Train a neuron with 3 inputs and 1 output on the data.\n",
    "\n",
    "(iii) Can you find a good way to visualize the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
